{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8291, 0.4095, 0.3342, 0.4797, 0.5741],\n",
      "          [0.2041, 0.6335, 0.5849, 0.1617, 0.3865],\n",
      "          [0.5455, 0.1099, 0.5336, 0.6176, 0.4499],\n",
      "          [0.4346, 0.4632, 0.4836, 0.3452, 0.7406],\n",
      "          [0.1344, 0.5163, 0.2153, 0.8363, 0.2419]]],\n",
      "\n",
      "\n",
      "        [[[0.7645, 0.3519, 0.4481, 0.3072, 0.6368],\n",
      "          [0.3275, 0.2964, 0.9426, 0.7861, 0.7417],\n",
      "          [0.7721, 0.7340, 0.5086, 0.4126, 0.3882],\n",
      "          [0.4816, 0.3722, 0.3667, 0.8580, 0.6363],\n",
      "          [0.3358, 0.5629, 0.7267, 0.6127, 0.3422]]]])\n",
      "tensor([[[[0.6690]]],\n",
      "\n",
      "\n",
      "        [[[0.7541]]]])\n"
     ]
    }
   ],
   "source": [
    "# verify dropout function  neck50\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = torch.rand([2,2,5,5])\n",
    "attention = torch.mean(a, dim=1, keepdim=True)\n",
    "max_val,_ = torch.max(attention.view(2,-1),dim=1,keepdim=True)\n",
    "thr = max_val * 0.8\n",
    "thr = thr.view(2,1,1,1)\n",
    "mask = (attention < thr).detach().cpu().numpy()\n",
    "# cv2.imshow('mask',mask[0].squeeze())\n",
    "# cv2.waitKey()\n",
    "plt.imshow(mask[0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# print(a)\n",
    "print(attention)\n",
    "print(thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([2,3])\n",
    "b = torch.tensor([3,2])\n",
    "print(torch.mul(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.38061092 0.7614966  0.78760615 0.93694619 0.52193596]\n",
      "  [0.36973036 0.9501027  0.20694673 0.46166495 0.15631667]\n",
      "  [0.83148153 0.24184182 0.35977547 0.29899741 0.22063775]\n",
      "  [0.83964191 0.75910764 0.41658329 0.76769669 0.65013374]\n",
      "  [0.26541767 0.86917096 0.27072457 0.96891331 0.16876316]]\n",
      "\n",
      " [[0.4246824  0.56089967 0.27868546 0.31766894 0.44208194]\n",
      "  [0.91998606 0.18128022 0.42837323 0.50828115 0.00950295]\n",
      "  [0.78695838 0.87674686 0.31032887 0.74164673 0.12670042]\n",
      "  [0.31892343 0.78580008 0.24804218 0.06919116 0.44310393]\n",
      "  [0.83846524 0.71735934 0.21136386 0.38087683 0.50277661]]]\n",
      "tensor(1.)\n",
      "[[[0.73105859 0.73105859 0.78760615 0.93694619 0.52193596]\n",
      "  [0.73105859 0.73105859 0.20694673 0.46166495 0.15631667]\n",
      "  [0.83148153 0.24184182 0.35977547 0.29899741 0.22063775]\n",
      "  [0.83964191 0.75910764 0.41658329 0.76769669 0.65013374]\n",
      "  [0.26541767 0.86917096 0.27072457 0.96891331 0.16876316]]\n",
      "\n",
      " [[0.4246824  0.56089967 0.27868546 0.31766894 0.44208194]\n",
      "  [0.91998606 0.18128022 0.42837323 0.50828115 0.00950295]\n",
      "  [0.78695838 0.87674686 0.31032887 0.74164673 0.12670042]\n",
      "  [0.31892343 0.78580008 0.24804218 0.06919116 0.44310393]\n",
      "  [0.83846524 0.71735934 0.21136386 0.38087683 0.50277661]]]\n"
     ]
    }
   ],
   "source": [
    "# soft_label.py \n",
    "\n",
    "import cv2\n",
    "import numpy as  np\n",
    "\n",
    "def _sigmoid(x):\n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1+z)\n",
    "    return (sig)\n",
    "\n",
    "    \n",
    "a = np.random.rand(2,5,5)\n",
    "print(a)\n",
    "rect = np.array([[0,0],[0,1],[1,1],[1,0]], np.int32)\n",
    "b = torch.tensor(1,dtype=torch.float32)\n",
    "print(b)\n",
    "score = _sigmoid(np.array(b))\n",
    "cv2.fillConvexPoly(a[0], rect, score)\n",
    "print(a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9820137900379085\n"
     ]
    }
   ],
   "source": [
    "# soft_label.py  verify sigmoid function\n",
    "# sigmoid function\n",
    "\n",
    "def sigmoid(x):\n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1+z)\n",
    "    return sig\n",
    "\n",
    "x = 4\n",
    "sig = sigmoid(x)\n",
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "mask_score = np.zeros([2,5,5])\n",
    "mask_score[0,:,:] = 1\n",
    "print(mask_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0292, 0.7055, 0.0093, 0.4838, 0.6822],\n",
      "        [0.6524, 0.9393, 0.6565, 0.7506, 0.7321],\n",
      "        [0.5173, 0.0902, 0.9025, 0.0767, 0.2525]])\n",
      "tensor([[0.2637, 0.3560, 0.5627, 0.6538, 0.2305],\n",
      "        [0.1933, 0.4308, 0.3014, 0.1799, 0.7130],\n",
      "        [0.5749, 0.7855, 0.5649, 0.3850, 0.9646]])\n",
      "tensor([[0.1343, 0.2642, 0.1317, 0.2117, 0.2581],\n",
      "        [0.1811, 0.2412, 0.1818, 0.1998, 0.1961],\n",
      "        [0.2206, 0.1439, 0.3242, 0.1420, 0.1693]])\n",
      "tensor([0.4739, 0.6126, 1.5227])\n",
      "weight tensor([0.3560, 0.4308, 0.7855])\n",
      "tensor(0.8697)\n"
     ]
    }
   ],
   "source": [
    "#verify soft loss\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "class SoftCrossEntropyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 reduction='mean',\n",
    "                 class_weight=None,\n",
    "                 bce_use_sigmoid=False,\n",
    "                 loss_weight=1.0):\n",
    "        \"\"\"CrossEntropyLoss.\n",
    "\n",
    "        Args:\n",
    "            use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n",
    "                of softmax. Defaults to False.\n",
    "            use_mask (bool, optional): Whether to use mask cross entropy loss.\n",
    "                Defaults to False.\n",
    "            reduction (str, optional): . Defaults to 'mean'.\n",
    "                Options are \"none\", \"mean\" and \"sum\".\n",
    "            class_weight (list[float], optional): Weight of each class.\n",
    "                Defaults to None.\n",
    "            loss_weight (float, optional): Weight of the loss. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        super(SoftCrossEntropyLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "        self.class_weight = class_weight\n",
    "        self.bce_use_sigmoid = bce_use_sigmoid\n",
    "\n",
    "    def forward(self,\n",
    "                cls_score,\n",
    "                label,\n",
    "                label_,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            cls_score (torch.Tensor): The prediction. shape is (N ,C)\n",
    "            label (torch.Tensor): The learning label of the prediction. shape is (N, C)\n",
    "            weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "            avg_factor (int, optional): Average factor that is used to average\n",
    "                the loss. Defaults to None.\n",
    "            reduction (str, optional): The method used to reduce the loss.\n",
    "                Options are \"none\", \"mean\" and \"sum\".\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated loss\n",
    "        \"\"\"\n",
    "        # cls_score = torch.sigmoid(cls_score)\n",
    "        # print(cls_score)\n",
    "        # loss = []\n",
    "        # for i in range(cls_score.shape[1]):\n",
    "        #     loss.append(-1 * label[:,i] * torch.log(cls_score[:,i]))\n",
    "        \n",
    "        # loss = sum(loss)\n",
    "        # print(loss)\n",
    "        # loss = torch.sum(loss) / avg_factor\n",
    "        cls_score = torch.softmax(cls_score, dim=1)\n",
    "        print(cls_score)\n",
    "        loss = []\n",
    "        index = label_ >= 0\n",
    "        \n",
    "        # for i in range(cls_score.shape[1]):\n",
    "        #     loss.append(-1 * torch.abs(label[:,i] - cls_score[:,i]) * label[:,i] * torch.log(cls_score[:,i]))\n",
    "        # loss = sum(loss)\n",
    "        # loss =  (label[index,label_] - cls_score[index,label_]) * label[index,label_] * torch.log(label[index,label_]/(cls_score[index,label_]+1e-6)+1e-6)\n",
    "        loss = -1* label[index,label_] * torch.log(cls_score[index, label_])\n",
    "        print(loss)\n",
    "        print('weight', label[index,label_])\n",
    "        loss = torch.sum(loss) / avg_factor\n",
    "        return loss\n",
    "\n",
    "pred = torch.rand([3,5])\n",
    "print(pred)\n",
    "label = torch.rand([3,5])\n",
    "print(label)\n",
    "index = torch.ones([3,1], dtype=torch.long).reshape(-1)\n",
    "\n",
    "# index = index>0\n",
    "# print(pred[:,0])\n",
    "mask_loss = SoftCrossEntropyLoss()\n",
    "loss = mask_loss(pred, label,index, avg_factor = pred.shape[0])\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n",
      "tensor([1, 0, 1, 1])\n",
      "tensor([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.tensor([1,0,1,1])\n",
    "c = a>=0\n",
    "print(c)\n",
    "print(a)\n",
    "b = torch.tensor([[0,1],[3,3],[0,1],[0,1]])\n",
    "print(b[c,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6372, 0.3279, 0.1948, 0.2485],\n",
      "        [0.9321, 0.5441, 0.5144, 0.5800]])\n",
      "tensor([[0.6541, 0.5812, 0.5486, 0.5618],\n",
      "        [0.7175, 0.6328, 0.6258, 0.6411]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([2,4])\n",
    "print(a)\n",
    "print(torch.sigmoid(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "for i in a:\n",
    "    if i<3:\n",
    "        print(i)\n",
    "    elif i<5:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9722, 0.2687, 0.3272, 0.4681, 0.9185],\n",
      "        [0.4625, 0.0172, 0.5045, 0.3006, 0.1448],\n",
      "        [0.6476, 0.9461, 0.1008, 0.2640, 0.1089],\n",
      "        [0.3061, 0.6971, 0.7300, 0.1348, 0.1165]])\n",
      "tensor([[0.9722, 0.2687, 0.3272, 0.4681, 0.9185],\n",
      "        [0.4625, 0.0172, 0.5045, 0.3006, 0.1448],\n",
      "        [0.6476, 0.9461, 0.1008, 0.2640, 0.1089],\n",
      "        [0.3061, 0.6971, 0.7300, 0.1348, 0.1165],\n",
      "        [0.3669, 0.5706, 0.1047, 0.5495, 0.1103],\n",
      "        [0.8056, 0.0066, 0.4653, 0.6637, 0.4176],\n",
      "        [0.7744, 0.6122, 0.4593, 0.3472, 0.1928],\n",
      "        [0.0178, 0.4678, 0.3938, 0.3416, 0.0827]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand([4,5])\n",
    "c = torch.rand([4,5])\n",
    "\n",
    "print(a)\n",
    "b = torch.cat([a, c])\n",
    "# b = a[0][1]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f78042e5d2f54b95e8507b2f81fb6e9a995c3f06da42f7136bd30d8612d5df5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
